'use client';
import { useState, useRef, useCallback, useEffect } from 'react';

export default function AudioRecorder() {
  const [conversation, setConversation] = useState([]);
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [autoListenEnabled, setAutoListenEnabled] = useState(false);
  
  const audioRef = useRef(null);
  const recorderRef = useRef(null);
  const chunksRef = useRef([]);
  const chatEndRef = useRef(null);

  // Handle TTS playback state
  useEffect(() => {
    const audio = audioRef.current;
    
    const handlePlay = () => {
      setIsSpeaking(true);
      if (autoListenEnabled) {
        setupAutoListenAfterTTS();
      }
    };
    
    const handleEnd = () => {
      setIsSpeaking(false);
      if (autoListenEnabled) {
        startRecording(); // Auto-start listening after TTS finishes
      }
    };
    
    audio?.addEventListener('play', handlePlay);
    audio?.addEventListener('ended', handleEnd);
    
    return () => {
      audio?.removeEventListener('play', handlePlay);
      audio?.removeEventListener('ended', handleEnd);
    };
  }, [autoListenEnabled]);

  const setupAutoListenAfterTTS = () => {
    // This will be called when TTS starts playing
    // We'll set up a listener to detect when it ends
    const audio = audioRef.current;
    if (!audio) return;

    const checkStillPlaying = () => {
      if (!audio.paused && !audio.ended) {
        setTimeout(checkStillPlaying, 100);
      } else {
        if (autoListenEnabled) {
          startRecording();
        }
      }
    };

    checkStillPlaying();
  };

  const processConversation = useCallback(async (audioBlob) => {
    if (!(audioBlob instanceof Blob)) {
      console.error('Invalid audio blob');
      return;
    }

    setIsProcessing(true);
    
    try {
      // STT Step
      const sttFormData = new FormData();
      sttFormData.append('audio', audioBlob, `recording-${Date.now()}.webm`);
      
      const sttResponse = await fetch('/api/transcribe', {
        method: 'POST',
        body: sttFormData,
      });

      if (!sttResponse.ok) throw new Error('STT failed');
      const { text: userText } = await sttResponse.json();
      
      // Add user message to conversation
      const userMessage = { 
        speaker: 'user', 
        text: userText,
        timestamp: new Date().toLocaleTimeString()
      };
      setConversation(prev => [...prev, userMessage]);
      
      // Auto-scroll to new message
      setTimeout(() => {
        chatEndRef.current?.scrollIntoView({ behavior: 'smooth' });
      }, 0);
      
      // LLM Step
      const llmResponse = await fetch('/api/llm', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          prompt: userText,
          conversationHistory: conversation
        }),
      });

      if (!llmResponse.ok) throw new Error('LLM failed');
      const { response: aiText } = await llmResponse.json();
      
      // Add AI response to conversation
      const ashleyMessage = { 
        speaker: 'ai', 
        text: aiText,
        timestamp: new Date().toLocaleTimeString()
      };
      setConversation(prev => [...prev, ashleyMessage]);
      
      // Auto-scroll to new message
      setTimeout(() => {
        chatEndRef.current?.scrollIntoView({ behavior: 'smooth' });
      }, 0);
      
      // TTS Step
      const ttsResponse = await fetch('/api/tts', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: aiText }),
      });

      if (!ttsResponse.ok) throw new Error('TTS failed');
      const responseBlob = await ttsResponse.blob();
      const audioUrl = URL.createObjectURL(responseBlob);
      
      if (audioRef.current) {
        audioRef.current.src = audioUrl;
        audioRef.current.play().catch(e => console.error('Audio play failed:', e));
        audioRef.current.onended = () => {
          URL.revokeObjectURL(audioUrl);
        };
      }

      // Enable auto-listening after first successful interaction
      if (!autoListenEnabled) {
        setAutoListenEnabled(true);
      }

    } catch (error) {
      console.error('Conversation error:', error);
      setConversation(prev => [...prev, { 
        speaker: 'ai', 
        text: 'Sorry, I encountered an error. Please try again.',
        timestamp: new Date().toLocaleTimeString()
      }]);
    } finally {
      setIsProcessing(false);
    }
  }, [conversation, autoListenEnabled]);

  const startRecording = async () => {
    if (isProcessing) return;
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      
      recorderRef.current = new MediaRecorder(stream);
      chunksRef.current = [];
      
      recorderRef.current.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };
      
      recorderRef.current.start(100);
      setIsRecording(true);
    } catch (error) {
      console.error('Recording error:', error);
      setConversation(prev => [...prev, { 
        speaker: 'system', 
        text: 'Microphone access error. Please allow microphone permissions.',
        timestamp: new Date().toLocaleTimeString()
      }]);
    }
  };

  const stopRecording = async () => {
    if (!recorderRef.current) return;
    
    return new Promise((resolve) => {
      recorderRef.current.onstop = () => {
        const audioBlob = new Blob(chunksRef.current, { type: 'audio/webm' });
        processConversation(audioBlob).finally(resolve);
        
        recorderRef.current.stream.getTracks().forEach(track => track.stop());
      };
      
      recorderRef.current.stop();
      setIsRecording(false);
    });
  };

  const clearConversation = () => {
    setConversation([]);
    setAutoListenEnabled(false); // Reset auto-listen when clearing conversation
  };

  const handleMicButtonClick = async () => {
    if (isRecording) {
      await stopRecording();
    } else {
      await startRecording();
    }
  };

  return (
    <div className="flex flex-col h-full gap-4">
      <div className="flex-1 overflow-y-auto space-y-4 p-2">
        {conversation.length === 0 ? (
          <div className="text-center py-8 text-base-content/60">
            <div className="text-5xl mb-4">üéôÔ∏è</div>
            <h3 className="text-xl font-medium">Talk to Ashley</h3>
            <p className="mt-1">Press the microphone button to start</p>
          </div>
        ) : (
          conversation.map((entry, index) => (
            <div 
              key={index} 
              className={`flex ${entry.speaker === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <div className={`max-w-xs md:max-w-md rounded-lg p-4 ${
                entry.speaker === 'user' 
                  ? 'bg-primary text-primary-content' 
                  : entry.speaker === 'ai'
                    ? 'bg-accent text-accent-content'
                    : 'bg-secondary text-secondary-content'
              }`}>
                <div className="text-xs opacity-80 mb-1">
                  {entry.speaker === 'user' ? 'You' : 
                   entry.speaker === 'ai' ? 'Ashley' : 'System'} ‚Ä¢ {entry.timestamp}
                </div>
                {entry.text}
              </div>
            </div>
          ))
        )}
        <div ref={chatEndRef} />
      </div>

      <audio ref={audioRef} className="hidden" />

      <div className="flex justify-center gap-4">
        <button
          onClick={clearConversation}
          className="btn btn-ghost"
          disabled={conversation.length === 0 || isRecording || isProcessing}
        >
          Clear Chat
        </button>
        <button
          onClick={handleMicButtonClick}
          className={`btn btn-circle btn-lg ${
            isRecording ? 'btn-error animate-pulse' : 'btn-primary'
          }`}
          disabled={isProcessing}
        >
          {isProcessing ? (
            <span className="loading loading-spinner"></span>
          ) : isRecording ? (
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
              <rect x="6" y="4" width="4" height="16" rx="1"></rect>
              <rect x="14" y="4" width="4" height="16" rx="1"></rect>
            </svg>
          ) : (
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
              <path d="M12 2a3 3 0 0 0-3 3v7a3 3 0 0 0 6 0V5a3 3 0 0 0-3-3Z"></path>
              <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
              <line x1="12" x2="12" y1="19" y2="22"></line>
            </svg>
          )}
        </button>
      </div>

      {/* Status indicator */}
      <div className={`fixed bottom-20 left-1/2 transform -translate-x-1/2 
        ${isRecording ? 'bg-red-500' : 'bg-gray-500'} 
        rounded-full w-4 h-4 transition-all duration-300
        ${isRecording ? 'animate-pulse' : ''}`}
        title={isRecording ? 'Listening...' : autoListenEnabled ? 'Ready to listen' : 'Press mic to start'}
      />
    </div>
  );
}